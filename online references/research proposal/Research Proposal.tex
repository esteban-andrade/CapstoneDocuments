\documentclass[12pt]{report}
\usepackage[a4paper,left=25mm, right=25mm, top=30mm, bottom=30mm]{geometry}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{microtype}
\usepackage{setspace}
\usepackage{enumitem}


\usepackage[nottoc]{tocbibind}
\usepackage{natbib}
\usepackage[section]{placeins}

\usepackage{graphicx}
\usepackage{lscape}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subfig}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{fancyhdr}
\usepackage{natbib}
\captionsetup{margin=10pt,font=small,labelfont=bf}
\usepackage[raggedright]{titlesec}
\usepackage{amsmath}
\usepackage{titlesec}
\usepackage{lipsum}
\usepackage{parskip} 


\renewcommand{\familydefault}{\sfdefault}
\setstretch{1.25}
\graphicspath{ {./images/} }
\bibliographystyle{agsm} %agsm

\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\huge}
\begin{document}
\begin{titlepage}

   

    \title{ \includegraphics[scale=1.7]{utslogo.jpg}\\[1cm]  
    Faculty of Engineering and Information Technology\\[1.0cm] 
    \Large{\textbf{Human Body 3D Scanner (Virtual me)}}\\}
    \author{Esteban Andrade\\ 
    12824583\\
    Supervisor: Dr Teresa Vidal Calleja\\}
    \date{\today}  
     \maketitle
     \cleardoublepage
\end{titlepage}

\addtocontents{toc}{\protect\thispagestyle{empty}}
\tableofcontents
\thispagestyle{empty}
\newpage
\setcounter{page}{1}

\chapter{Engineering Reseach Problem}

\section{Reseach Question}
\textit{\large{"Human Body 3D Scanner: The development of software for 3D data reconstruction of a Human body scanner with multiple sensors" }}

\section{Project Contextualization}
The project is based on creating a Human Body 3D scanner.
It will have two specific streams that include the development of  the mechatronic design of a 3D scanner for a human and the software development for 3D data reconstruction. 
This proposal is based on developing the software for 3D data modelling and reconstruction of the Scanned data.\\[10pt]
Similarly, with the 3D reconstructed model of the human has the aim to be utilised to test different fashion clothing items. This has the intent to adjust the sizing of the clothes fittings based on the Scanned data. The clothing models will adjust automatically depending on the dimensions of the data of the scanned model. 
   
\section{Problem Definition}
Being able to obtain data from multiple sensors and model objects has is very crucial for many industries.
Nevertheless, there is a lack of precise and accurate options in the market that could create 3D models of Humans with respective sensor data. Similarly there are no current industry application that maximise the potential use of the Human body 3D models.
Therefore, this project is aimed at creating a solution and develop the software for 3D data reconstruction of the scanned human. This model will be utilised to try different fashion items and adapt the size fittings accordingly. \\[10pt]
The project will have different stages that range from testing different sensors for data acquision, testing different data stitching frameworks to the deployment of the software in the 3D scanner mechatronic device. 
\enlargethispage{\baselineskip}

\section{Background}
The human society has the world comprenhension of the surrouding world through visual perception. This principle allows to differenciate distintive kinds of shapes, objects,colours, textures and the spatial pose of the surroudings.
Based on this information, it is possible to analyse the number of objects in a determined location, object type, object size, object pose in different coordinate frames. 
Thus, it impacts how as a society we interactuate with objects ot scenes. As a result it is essential to imitate this perception in order to acquire real world data in different formats that include:
\begin{itemize}[]
    \item RGB images
     \item Depth images
     \item 3D point clouds 
     \item Multispectral images
     \item Laser readings
\end{itemize}
All these acquire data can be obtained from a wide variety of comercial or industrial sensors. With this data it will be possible to use computer processing techniques in order to model the object or scene \citep*{murcia_monroy_mora_2018}.

\section{Applications}
In the recents years the use of 3D body scanners has gain importance in several industries. Within the fashion industry it can aid clothes manufactures to obtain accurate body measurement data of body dimensions.
As mentioned by \citet*{sturm_bylow_kahl_cremers_2013}, this new technological approach has the potential to alterate the future of the fashion and clothing manufacturing industry.

With the raise of innovatoin of 3D image reconstruction, the interest from to gather precise measurements of the human has raised. Due to the fact, that in the clothing industry
is extremedely important to create better fittings for different shapes of human bodies. 
Furthermore, virtual try-on solutions has gain popularity in physical and online retail stores \Citep*{spahiu_shehi_piperi_2014}.

On the other side 3D scanners have gain in particupation in the medical industry. These systems are described as "non-invasive and low cost" , thus making it appealing for epidemiological surveys and clinical uses. \Citep*{treleaven_wells_2007}
The geometrical measurements could be associated with shape, size, volume and surface area of the body parts. It could aid to be a sustainable approach to screen children and patients with obesity, deformities or specific anatomic defects. 
Therefore, it will ease the diagnose process and allow to treat and monitor medical conditions holisticly and improve the life quality of patients with non-invasive tests.
The table below illustrates the use of 3D scanner in the medical field with the purpose to identify and monitor various medical conditions. 
Frpm which the diagnose, treatment and monitor procedures willd differ based on the acquired data.
\begin{table}[ht]
    \centering
    \includegraphics[width=15cm]{table1.png}
    \caption{3D Scanning Applications \cite[]{treleaven_wells_2007}.}
\end{table}

\begin{figure}[h]
  \begin{center}
  \includegraphics[width=0.5\textwidth]{bodyMedicine.png}
  \caption{Front, Side results of 3D Scanning \cite[]{treleaven_wells_2007}.}
  \label{bodymed}
\end{center}
\end{figure}


\chapter{Related Works}
Being able to scan different objects and subjects has been challeging task for researchers. Getting an accurate spatial location of the objects is crucial for this  type of application.
The use of 3D point clouds has facilitated this process as it allows to obtain the following parameters:
\begin{itemize}[]
  \itemsep0em 
  \item Depth
  \item Intensity
  \item Pulse width
  \item Light echo
\end{itemize}
This information can be obtain with different kind of sensors. There is a wide variety of off the shelf sensors that can procide 3D point clouds. 
These sensors could either be stereo or multiview vision cameras, lasers, time-of-flight sensors (\textit{TOF}) and structured light sensors as stated by \Citet*{murcia_monroy_mora_2018}.

Many Scanning devices will use single or multiple of the above-mentioned sensors in order to acquire data. Once the data is obtained, it essential to have a framework for 3D data modelling and reconstruction.
The principle behind 3D data recontruction is obtained with data fision from RGB-D sensors. This kind of sensors provide 3 channels images RGB (red,green, blue) and the depth images are mapped to each pixel. Based on this data 3D point clouds could be generated for data recontruction.
One of the most common frameworks is known as \textbf{Dynamic Fusion} which is referenced to \textit{"reconstruction and tracking of Non-rigid Scenes in real time"} \Citep*{newcombe_fox_seitz_2015}.
Another recent powerful 3D Data recontruction framwork is \textbf{SurfelWarp} which is defined as \textit{"Efficient Non-Volumetic Single View Dynamic Reconstruction"} \Citep*{SurfelWarp}.


\section{Dynamic Fusion}
Dynamic fusion is the based on three different technologies focused on 3D scanning and data recontruction. These techniques are: 

\begin{description}[style=nextline]
    \item[DART (Dense Articulated Real Time Tracking)] This technology is speciaclised  on Real Time body template skeleton tracking.
    \item[Animation Cartography] It is a 3D reconstruction technique  focused on intrinsic data reconstruction of shapes and motions. 
    \item[Kinect Fusion] This technology is applied for real-time tracking and condence surface mapping.It is intended to be used in static scenes and objects with only a moving camera sensor. 
\end{description} 

\vspace{5mm}

\begin{wrapfigure}{r}{0.5\textwidth} %this figure will be at the right
    \centering
    \includegraphics[width=0.5\textwidth]{IMG_0073.png}
    \caption{Transformation Between Depth Video to Warped canonical Model. \cite[]{newcombe_fox_seitz_2015}}
\end{wrapfigure}
The principal  focal point of the 3D data recontruction feature of \textit{Dynamic Fusion}  is that it will look for a solution for for the volumetric flow based on the gathered data.
As mentioned by \Citet*{newcombe_fox_seitz_2015} there will be a transformation of the state ot the scene at each time interval to a fixed canonical frame.
The created  canonical frame is described as the initial frame that is obtained from the non-rigid object that has been detected and tracked.The shape of the detected object is defined as "canonical model" which is the correspoding shape of the object in the canonical frame. 
Therfore, the canonical model will be utilised as reference model for all the subsequent frames. From this approach there will be progressing adjustment on on the canonical model and frames, as more data is acquired.
With the new refinements each point in the canonical frame, the point clouds will be transformed and updated to the new location in real time based on the received data.

The data acquired from sensors that includes RGB and Depth images will help to determine the warp parameters. Based on the determined warp parameters the volumetric flow field can be estipulated.
The state of the wrap field W\textsubscript{t} is defined as a function of time. It is modelled by the values of a set of \textit{"n"} deformation nodes, which are described as the points or pixels in the actual image.
The image below describes the process in which the canonical model and frame are determined based on the initial frame and depth data. 
\begin{figure}{}%this figure will be at the right
    \centering
    \includegraphics[width=0.7\textwidth]{dynamicfusion2.png}
   \caption{Dynamic Fusion methodology \cite[]{newcombe_fox_seitz_2015}}
\end{figure}


The State of the warp field W\textsubscript{t} can be modelled with the below equation.
\[N_{warp}^t=\{dg_v,dg_w,dg_{se3}\}t\]
\begin{itemize}[label =]
    \item $dg_{v}$: is described as the 3D position of each node in the canonical frame. 
    \item $dg_{se3}$: is the Special Euclidian transformation where $T_{ic}=dg_{se3} ^ {i}$ is the rigid transformation for every node $i$.
    \item $dg_{w}$: It controls the extend impact of the deformation around each node.
\end{itemize}

The current set of point clouds will be stored  as a "polygon mesh" with the normal pair of points within the canonical frame and allow to calculate the warp field parameters. 
The principle will allow effective surface reconstruction as suggested by \Citet*{slavcheva_baust_ilic_2018} as once the warp field parameters are obtained, surface recontruction can be modelled with a principle of marching cubes.
This process will be followed by a rasterization rendering pipeline of the Acquired Point cloud values \Citep*{newcombe_fox_seitz_2015}.

The patterns from the figure \ref{fig:patterns} illustrate the triangulated cubes for the 15 basic patterns used in marching cubes for surface recontruction.
These patterns are able to reconstruct all  256 possible solutions using rotational and  complementary symmetry as suggested by \Citet*{fang_zhao_wen_zhang_2018}.
Once the canonical model and frame can be modelled with the warp field parameters based from the intial raw depth image maos ad data frame, the tracking nodes will be created.
Based on this it is possible to obtain the canonical frame warp parameters which  are estimated based on this process. 
As soon as the canonical model is constructed, the life frame will be warp around it based on the warp parameters. As a result the model will be 3D reconstructed model will be succesfully created and normalized.
\begin{figure} %this figure will be at the right
    \centering
    \includegraphics[width=0.4\textwidth]{shapes.png}
    \caption{Triangulated patterns \cite[]{fang_zhao_wen_zhang_2018}}
    \label{fig:patterns}
\end{figure}

\enlargethispage{\baselineskip}

\section{SurfelWarp}
SurfelWarp is defined as "Efficient Non-Volumetric Single View Dynamic Reconstruction. It will present a standard graphics pipeline and GPGPU computing can be utilised for efficient 
implementation of all data recontruction operations \Citep*{SurfelWarp}. It eliminates the use of volumetric data structures, which represent resouce intensive volumetric operations such as dense deformation field updates, volumetric fusion and marching cubes.
This represents a significant performance improvement as the explicit surfel representation allow to directly recover from tracknig failures or topology changes as proposed by \citeauthor*{SurfelWarp}.

\subsection{Overview}
As illustrated in figure \ref{fig:surfelmetho}, SurfelWarp is built in a frame by frame methodology to process an input depth stream data source. 
When a new depth image is received, the deformation field that is aligned to the reference frame geometry will be solved. 
This is calculated by startting the deformation field from the previous image, which is followed by an iterative optimization problem similar to \Citet*{newcombe_fox_seitz_2015}.
Once the deformation field is update, the data fusion process is carried out. This will trigger an accumulative process to fuse the current depth observations into a geometrical representation. 

The deformation field $ W = \{[p_j \in R^{3},\sigma_{j} \in R^{+} ,T_j \in SE(3)]\}$ where j is the node index, $p_j$ is the position of the $j^{th}$ node. $\sigma_j$ is the radius parameter, $T_j$ is the 6DoF transformation.
For any point in "x" the deformation can be interpolated by equation \ref{eq:deffield1}

\newpage
\begin{figure}%this figure will be at the right
    \centering
    \includegraphics[width=0.95\textwidth]{surfelwarp1.png}
    \caption{SurfelWarp methodology \cite[]{SurfelWarp}}
    \label{fig:surfelmetho}
\end{figure}


\begin{equation}  
  W(x)=normalized(\Sigma_{k \in N(x)} w_{k}(x) \hat{q_k})
  \label{eq:deffield1}
\end{equation}
From equation \ref{eq:deffield1} the following components can be described:
\begin{itemize}
  \item $N(x)$: is the closest set of points x
  \item $W_{k}(x)$:  is the weight that can be computed as $\exp{(-\frac{\ | {x-p_k} \ |^{2}_2 }{2\sigma^{2}_k})}$
\end{itemize}

A surfel $S$ could is described as the tuple composed in which the following components can be modelled :
\begin{itemize}
  \item position : $v \in R^3$ 
  \item  normal  : $n \in R^3$
  \item  radius : $r \in R^+$
  \item confidence : $c \in R$
  \item initilization time : $t_{init} \in N$
  \item most recent time : $t_{observed} \in N$ 
\end{itemize}
Therefore a surfel can be illustratedby the deformatin of field $W$ in equation \ref{eq:deffield1} \citep{SurfelWarp}.
Furthermore the deformed vertex position  and normal can be modelled with the following equations:
\begin{equation}
  v_{life}=W(v_{ref})v_{ref}
  \label{eq:deformed vertex}
\end{equation}
\begin{equation}
  n_{life}=rotation(W(v_{ref})n_{ref})
  \label{eq:deformed vertex normal}
\end{equation}

From equations \ref{eq:deformed vertex} and \ref{eq:deformed vertex normal} $v_{life}$ and $n_{life}$ are the deformed vertex position and normal.
Whereas $v_{ref}$ and $n_{ref}$ correspond to the vertex position and normal before the deformation process. 


\subsection{Depth Map Fusion \& Warp Fiel Update}
In order to get the warp fiel estimate, it is neccesary to perform a mathematical prediction of the visibility of the live surfels models $S_{life}$ with the proposed method of \Citet*{newcombe_fox_seitz_2015}
to cast the deformation estimation into a optimization problem.
The estimation process is performed by predicting the visibility of the life surfels $S_{life}$.
\begin{figure}[h]%this figure will be at the right
  \centering
  \includegraphics[width=0.9\textwidth]{surfelwarp2.png}
  \caption{SurfelWarp Reconstruction\cite[]{SurfelWarp}}
  \label{fig:surfelprocess}
\end{figure}

Similarly, to the approach of \citet{keller} the life surfels $S_{life}$  are rendered as an overlaped disk shaped surface splat. These shapes are spanned by the position $V_{life}$ 
,normal $n_{life}$ and radius $r_{life}$ of the live surfel $s_{life}$.
With all these parameters it is possible to model the warp fiel based on the Dynamic Fusion framework proposed by \citet*{newcombe_fox_seitz_2015}.

Once the deformation is solved, the depth map fusion obtained from sensors along with the warp field update will perform data fusion in the live frame. 
This live frame  warps the live surfel back to the starting reference frame. Afterwards the warp field is recurrently updated based on the new observed surfels reference. This process can be exemplified with image \ref{fig:surfelprocess} .

\chapter{Methodology}



 \nocite{*}   % all not cited bib entrys are shown in bibliography ...
\bibliography{resources}
\end{document}
